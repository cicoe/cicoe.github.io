- name: "National Superconducting Cyclotron Laboratory"
  website: "https://www.nscl.msu.edu/"
  description: "
    The overall mission of the National Superconducting Cyclotron Laboratory (NSCL) at
    Michigan State University is to provide forefront research opportunities with stable
    and rare isotope beams. A broad research program is made possible by the large range
    of accelerated primary and secondary (rare isotope) beams provided by the facility.
    The major research thrust is to determine the nature and properties of atomic nuclei,
    especially those near the limits of nuclear stability. Other major activities are
    related to nuclear properties that influence stellar evolution, explosive phenomena
    in the cosmos (e.g. supernovae and x-ray bursts), and the synthesis of the heavy
    elements; and research and development in accelerator and instrumentation physics,
    including the development of superconducting radiofrequency cavities and design
    concepts for future accelerators for basic research and societal applications.
    In all activities an important part of the NSCL program is the training of the next
    generation of scientists. Upon completion of the DOE-funded Facility for Rare Isotope
    Beams (FRIB), the laboratory will transition to programs with beams from this facility.
    <br /><br />
    NSCL operates two coupled cyclotrons, which accelerate stable ion beams to energies
    of up 170 MeV/u. Rare isotope beams are produced by projectile fragmentation and
    separated in-flight in the A1900 fragment separator. For experiments with high-quality
    rare isotope beams at an energy of a few MeV/u, the high-energy rare isotope beams
    are transported to a He gas cell for thermalization, and then sent to the ReA
    linear post-accelerator for reacceleration. Rare isotope beams in this energy range
    allow nuclear physics experiments such as low-energy Coulomb excitation and transfer
    reaction studies as well as for the precise study of astrophysical reactions. The
    facility has produced over 904 rare isotope beams for experiments, and 65 new isotopes
    have been discovered at NSCL.
    <br /><br />
    NSCL is a national user facility and has a large user community with over 800 actual,
    active users in a given year. Most experiments conducted at NSCL involve international
    collaborations with about 75% of the experiments lead by a US spokesperson.
    <br /><br />
    NSCL provides beams to approximately 30 experiments per year. Experiments are short
    (~3-7 days) with many changes during and in between experiments. Data acquisition and
    analysis and simulation framework need to support fast online decision making.
    Experiments have increased significantly in complexity with an increase of the number
    of channels read out, often together with high-resolution digitized waveform data.
    Each experiment can generate up to 10 TB of experimental data set. Storage and backup
    systems must match such data sizes. Data sets are analyzed on-line during the data
    acquisition and later off-line either at NSCL or at the spokesperson's institution.
    Experiments with in-house spokespersons require long-term storage (usually a few years)
    of the full data set and adequate computing resources for analysis. A computing cluster
    in the order of 1000 cores dedicated for online analysis is foreseen. Network bandwidths
    of 100 Gbit/s will be required. External data transfer capabilities must continue to
    accommodate the needs of a large and distributed user community with increased data
    set sizes. Data sets are provided to experimenters via magnetic tape, though other
    methods are available.<br /><br />NSCL CI supports and enables the Laboratory overall
    mission. CI includes a broad range of functional areas: business support information
    technology, networking, accelerator controls, experimental controls and DAQ, and
    offline simulation and analysis. Internally developed and commercial solutions are
    used. Systems are primarily managed and maintained by Laboratory personnel. CI
    challenges include increasing security requirements, Laboratory growth with FRIB
    planning and construction, and increasing and foreseen experimental needs.
    <br /><br />
    The Business IT department provides a range of enterprise IT services directly
    supporting business processes including an internally hosted ERP suite and other
    customized COTS solutions. Windows based services including Active Directory, Exchange,
    SharePoint are deployed. More than 500 Windows desktop PCs are maintained.
    <br /><br />
    Business IT department also maintains the Lab-wide network, servers and storage
    used by DAQ and NSCL Controls and is responsible for overall IT security.
    <br /><br />
    Internet is provided via MSU with MSU assisting with Internet security. Laboratory
    wired networks are managed internally with MSU supporting wireless access.
    <br /><br />
    The Controls department is responsible for hardware and software controls for
    accelerators, beamlines, and other experimental equipment. The controls system
    uses EPICS protocols with graphical monitoring using CS-Studio. NSCL personnel
    are active in development of both projects. A number of associated systems provide
    alarms, access controls, archiving etc. for EPICS.<br /><br />With construction of
    the FRIB accelerator progressing, new accelerator and cryogenic controls networks
    are being deployed. These are also EPICS based. The designs emphasis security with
    FRIB Controls network isolated from other Laboratory systems.
    <br /><br />
    In house developed software forms the core of the DAQ systems. NSCLDAQ is a modular
    system supporting a range of experiment arrangements. SpecTcl is a compatible
    analysis software. DDAS is an internally developed digital-DAQ, supporting XIA
    Pixie-16 Digitizer and compatible with NSCLDAQ. As a user facility, NSCL provides
    DAQ assistance to visiting experimenters. Typical experiments produce approximately
    100 GB of data per day with experiments storing digitized waveforms producing ~1 TB
    per day. Currently, most experiments’ needs are met with 1GE networking and several
    DAQ computers. Data is recorded to ZFS/Linux servers. Reliability is critical as
    experiments' beam times are generally limited for less than one week. Visiting
    experimenters may make use of DAQ systems while present at NSCL.
    <br /><br />
    Increasingly, flexible CPU and software systems are used for DAQ. One purpose is
    distinguishing overlapping waveform signals from higher rate experiments. The
    GRETINA experiment is active at NSCL currently utilizing a dedicated farm of
    approximately 100 PC nodes (1000 cores) for selecting events based on digitized
    waveforms.
    <br /><br />
    Offline simulations and analysis systems are provided for Laboratory students,
    faculty and staff. Clustered interactive Linux hosts and a small (~50 node)
    Linux SLURM batch system are available. Approximately 1 PB of networked research
    storage is available using ZFS/Linux systems with NFS. Increasing detector complexity,
    data volumes and analysis complexity require increasing simulation and analysis
    capacity. Free and widely used applications such as ROOT and GEANT are the norm.
    "

- name: "The Cornell High Energy Synchrotron Source (CHESS)"
  website: "https://www.chess.cornell.edu/"
  description: "
    The Cornell High Energy Synchrotron Source (CHESS) is a NSF-funded National User Facility
    located on the Cornell University campus in Ithaca, New York.  The mission of CHESS is to
    provide a national hard x-ray synchrotron radiation facility for individual investigators,
    on a competitive, peer reviewed, proposal basis.  With 11 experimental stations, the facility
    is used by approximately 1,100 investigators per year from over 150 academic, industrial,
    government, non-profit, and international institutions.  CHESS impacts a wide range of
    disciplines, serving researchers from the physical, biological, engineering, and life
    sciences, as well as cultural specialists such as anthropologists and art historians.
    CHESS users conduct studies encompassing, but not limited to, the atomic and nanoscale
    structure, properties, operando, and time-resolved behavior of electronic, structural,
    polymeric and biological materials, protein and virus crystallography, environmental science,
    radiography  of  solids  and  fluids,  and  micro-elemental  analysis,  and  other
    technologies  for  x-ray science.
    <br /><br />
    The CHESS facility is hosted by the Cornell Laboratory for Accelerator-based Sciences
    and Education (CLASSE), which also operates the Cornell Electron Storage Ring (CESR) as
    the x-ray source for CHESS. Computing services for CHESS are provided centrally by the
    CLASSE-IT department.  The primary computing services used by CHESS are:
    <br />
    <ul>
      <li>high-speed data acquisition for x-ray detectors at the CHESS experimental stations</li>
      <li>access to and long-term storage of x-ray data collected by CHESS users</li>
      <li>software libraries and parallel computation resources for CHESS staff and users.</li>
    </ul>
    <br />
    <h4>CHESS Cyberinfrastructure</h4>
    The CLASSE cyberinfrastructure (CI) consists of an interconnected series of high-availability
    server clusters (HACs), data acquisition systems, control systems, compute farms, and workstations.
    Most of these systems run either Scientific Linux or Windows on commodity 64-bit Intel-based
    hardware and are centrally managed using Puppet.  The median age of key CI components is
    approximately 5 years, with an average refresh rate of once every 10 years.  The CLASSE CI
    components most relevant to CHESS are described below.
    <br /><br />
    <strong>Central Infrastructure</strong>
    <br />
    The  central  Linux  infrastructure  cluster  runs  the  core  CLASSE  infrastructure  services,
    including  name services, file systems, databases, and web services. Recently, a dedicated oVIrt
    cluster has been commissioned to run centrally-provisioned virtual machines.  These clusters
    utilize shared 10Gb iSCSI storage domains, and they provide file systems and other basic services
    to the rest of the lab.
    <br /><br />
    <h4>CHESS Data Acquisition (DAQ)</h4>
    The CHESS data acquisition system runs on a dedicated HAC and provides 10Gb network connections
    to each experimental station.  Data collected at the stations are written directly to the data
    acquisition system over  either  NFS  or  Samba,  where  it  can  then  be  processed  on  the
    CLASSE  Compute  Farm  or  end-user workstations.  CHESS users can also download their data
    remotely using a Globus server endpoint or via SFTP.
    <br /><br />
    <h4>Compute Farm</h4>
    The  CLASSE  Compute  Farm  is  a  central  resource  consisting  of  approximately  60
    enterprise-class  Linux nodes (with around 400 cores) with a front-end queueing system that
    distributes jobs across the Compute Farm nodes.  This queueing system supports interactive,
    batch, parallel, and GPU jobs, and it ensures equal access to the Compute Farm for all users.
    <br /><br />
    <h4>CESR Control System</h4>
    The CESR control system, responsible for running the particle accelerator that produces x-rays
    for CHESS, consists of a dedicated Linux HAC. Although the CESR, CLASSE, and CHESS DAQ clusters
    are essentially identical, the CESR cluster runs many more control system services and is able
    to operate independently from the CLASSE central infrastructure.  This isolation ensures
    continuity of CESR operations in the event of a power failure or general network outage.
    <br /><br />
    <h4>User Connectivity</h4>
    Based on their requirements, CHESS users are either granted restricted \"external\" CLASSE
    accounts (providing access to station computers and remote access to data) or full CLASSE
    accounts (providing access to the CLASSE Compute Farm and full interactive desktops, both
    local and remote).
    <br /><br />
    While  collecting  data  at  the  experimental  stations,  CHESS  users  generally  connect
    their  instruments and experimental equipment to a private subnet that is selectively
    firewalled from the rest of the CLASSE infrastructure.  If users require direct write access
    to the CHESS DAQ filesystems, they may use dedicated station and kiosk computers located at
    the experimental stations and in other restricted-access locations. Outside  the  experimental
    stations,  CHESS  user  data  is  made  available  for  read-only  access  through  the
    CLASSE public network.
    "

- name: "DesignSafe - Cyberinfrastructure for NSF Natural Hazards Engineering Research Infrastructure"
  website: "https://www.designsafe-ci.org/"
  description: "
    Natural hazards engineering plays an important role in minimizing the effects of natural
    hazards on society through the design of resilient and sustainable infrastructure. The
    DesignSafe cyberinfrastructure has been developed to enable and facilitate transformative
    research in natural hazards engineering, which necessarily spans across multiple disciplines
    and can take advantage of advancements in computation, experimentation, and data analysis.
    DesignSafe allows researchers to more effectively share and find data using cloud services,
    perform numerical simulations using high performance computing, and integrate diverse
    datasets such that researchers can make discoveries that were previously unattainable. This
    white paper describes the design principles used in the cyberinfrastructure development
    process, introduces the main components of the DesignSafe cyberinfrastructure, and
    illustrates the architecture of the DesignSafe cyberinfrastructure.
    <br /><br />
    A cyberinfrastructure is a comprehensive environment for experimental, theoretical, and
    computational engineering and science, providing a place not only to steward data from its
    creation through archive, but also a workspace in which to understand, analyze, collaborate
    and publish that data. Our vision is for DesignSafe to be an integral part of research
    and discovery, providing researchers access to cloud-based tools that support their work
    to analyze, visualize, and integrate diverse data types. DesignSafe builds on the core
    strengths of the previously developed NEEShub cyberinfrastructure for the earthquake
    engineering community, which includes a central data repository containing years of
    experimental data. DesignSafe preserves and provides access to the existing content from
    NEEShub and adds additional capabilities to build a comprehensive CI for engineering
    discovery and innovation across natural hazards. DesignSafe has been developed along
    the following principles:
    <br /><br />
    <strong>Create a flexible CI that can grow and change.</strong> DesignSafe is extensible,
    with the ability to adapt to new analysis methods, new data types, and new workflows
    over time. The CI is built using a modular approach that allows integration of new
    community or user supplied tools and allows the CI to grow and change as the disciplines
    grow and change.
    <br /><br />
    <strong>Provide support for the full data/research lifecycle.</strong> DesignSafe is not
    solely a repository for sharing experimental data, but is a comprehensive environment for
    experimental, simulation, and field data, from data creation to archive, with full support
    for cloud-based data analysis, collaboration, and curation in between. Additionally, it
    is the role of a cyberinfrastructure to continue to link curated data, data products, and
    workflows during the post-publication phase to allow for research reproducibility and
    future comparison and revision.
    <br /><br />
    <strong>Provide an enhanced user interface.</strong> DesignSafe supplies a comprehensive
    range of user interfaces that provide a workspace for engineering discovery. Different
    interface views that serve audiences from beginning students to computational experts
    allow DesignSafe to move beyond being a \"data portal\" to become a true research environment.
    <br /><br />
    <strong>Embrace simulation.</strong> Experimental data management is a critical need
    and vital function of the CI, but simulation also plays an essential role in modern
    engineering and must be supported. Through DesignSafe, existing simulation codes, as
    well as new codes developed by the community and SimCenter, are available to be invoked
    directly within the CI interface, with the resulting data products entered into the
    repository along with experimental and field data and accessible by the same analytics,
    visualization, and collaboration tools.
    <br /><br />
    <strong>Provide a venue for internet-scale collaborative science.</strong> As both digital
    data captured from experiments and the resolution of simulations grow, the amount of
    data that must be stored, analyzed and manipulated by the modern engineer is rapidly
    scaling beyond the capabilities of desktop computers. DesignSafe embraces a cloud
    strategy for the big data generated in natural hazards engineering, with all data,
    simulation, and analysis taking place on the server-side resources of the CI, accessible
    and viewable from the desktop but without the limits of the desktop and costly, slow
    data transfers.
    <br /><br />
    <strong>Develop skills for the cyber-enabled workforce in natural hazards engineering.</strong>
    Computational skills are increasingly critical to the modern engineer, yet a degree in
    computer science should not be a prerequisite for using the CI. Different interfaces
    lower the barriers to HPC by exposing the CI’s functionality to users of all skill
    levels, and best of breed technologies are used to deliver online learning throughout
    the CI to build computational skills in users as they encounter needs for deeper learning.
    <br /><br />
    The DesignSafe infrastructure provides a comprehensive environment for experimental,
    theoretical, and computational engineering and science, providing a place not only to
    steward data from its creation through archive, but also the workspace in which to
    understand, analyze, collaborate and publish that data. The CI can be described in
    terms of the services it provides or in terms of the technical components that enable
    those services.
    <br /><br />
    DesignSafe is architected to comprise the following services and components:
    <ul>
      <li><strong>DesignSafe</strong> front end web portal</li>
      <li>The <strong>Data Depot</strong>, a multi-purpose data repository for experimental, simulation, and field data that uses a flexible data model applicable to diverse and large data sets and is accessible from other DesignSafe components. The Data Depot includes an intelligent search capability that allows dynamic creation of catalogs of the held data in an easily understandable way, and that can search ill-structured data with poor or incomplete metadata.</li>
      <li>A <strong>Reconnaissance Integration Portal</strong> that facilitates sharing of reconnaissance data within a geospatial framework.</li>
      <li>A web-based <strong>Discovery Workspace</strong> that represents a flexible, extensible environment for data access, analysis, and visualization.</li>
      <li>A <strong>Learning Center</strong> that provides training and online access to tutorials.</li>
      <li>A <strong>Developer’s Portal</strong> that provides a venue for power users to extend the Discovery Workspace or Reconnaissance Integration Portal, and to develop their own applications to take advantage of the DesignSafe infrastructure’s capabilities.</li>
      <li>A foundation of <strong>storage and compute systems</strong> at the Texas Advanced Computing Center (TACC), to provide both on-demand computing and access to scalable computing resources.</li>
      <li>A <strong>middleware layer</strong> to expose the capabilities of the CI to developers, and to enable construction of diverse web and mobile interfaces to data products and analysis capabilities</li>
      <li>A marketplace of <strong>Community Defined Interfaces</strong>; the extension capability of the CI allows other projects to leverage DesignSafe to build an interface of their own choosing.</li>
    </ul>
    <br />
    The CI development was initiated in July 2015 upon receiving the NSF award, and was first deployed May 2016. As of June 2017 we have more than 1,100 registered users spanning dozens of institutions around the world.
  "

- name: "Daniel K. Inouye Solar Telescope National Solar Observatory"
  website: "http://dkist.nso.edu)/"
  description: "
    <h4>Introduction</h4>
    The Daniel K. Inouye Solar Telescope (DKIST) is a four-meter, off-axis Gregorian solar telescope currently under construction by the National Solar Observatory and AURA on Haleakala, Maui, Hawai’i. When complete in 2019, it will be the largest solar telescope in the world, providing facility-class, high-resolution solar observations to a small but growing community of students, researchers, and the general public. In full operations, planned to last fifty years, the DKIST will house five complex instruments and a state-of-the-art adaptive optics system, generating over three petabytes of raw data annually. Key to its success, then, is a cyberinfrastructure providing facility and instrument control, scientific and operational data acquisition, and data management, processing, and distribution services. In this whitepaper, we provide a high-level description of primary components of the cyberinfrastructure.
    <br /><br />
    <h4>Cyberinfrastructure</h4>
    The DKIST cyberinfrastructure is comprised of three primary components: the systems and infrastructure providing services to operate the telescope and its supporting subsystems (“Summit”), the core services and infrastructure needed to support science and engineering activities related to observatory operations and network services (“DKIST IT”), and the services and infrastructure performing long-term data management, processing, discovery, and distribution (“Data Center”). These components are highlighted in Figure 1, and discussed in more detail below.
    <br /><br />
    <strong>Summit</strong>. The DKIST Summit cyberinfrastructure comprises integrated facility, instrument control and safety systems, enabling telescope and dome control, optical alignment and routing, mechanical controls, observation execution and monitoring, instrument data acquisition, management, and distribution, and environmental monitoring and control. These systems are comprised of a High Level Software suite written primarily in Java and Python, utilizing CORBA. They are deployed through configuration-controlled provisioning stacks, including SaltStack, and sit atop an HPC architecture comprising many dedicated nodes interconnected through 10 Gb Ethernet and FDR InfiniBand. The Summit cyberinfrastructure is currently being readied for integration testing as a prelude to observatory integration efforts coming in the next 12-18 months.
    <br /><br />
    <strong>DKIST IT</strong>. The DKIST IT supports the observatory through deployment of core services such as routing, DNS, LDAP, and network maintenance and monitoring for the summit and a remote support building, as well ensuring SLAs and/or contracts with partner organizations (U. Hawai’I in Maui and U. Colorado in Boulder at the NSO Headquarters) are met and maintained. In addition, the DKIST IT provides operational support for physical infrastructure (optical fiber, Ethernet and InfiniBand networking, and routing hardware) on the Summit and the remote support building. Services are deployed through configuration-controlled provisioning stacks, sitting atop commodity equipment including Cisco switching. The DKIST IT is ramping its efforts, particularly with regard to network buildout on the Summit and the remote support facility.
    <br /><br />
    <strong>Data Center</strong>. The DKIST Data Center will provide long-term data management, scientific processing, search, and distribution services for the observatory. It will manage 3.2 PB of data per year, comprised of hundreds of millions of observations and tens of billions of metadata, exported by the Summit and, after calibration, intended for end-user consumption. Thus, data management and processing services must scale effectively with little rework, while data search depends on appropriate data modeling and well-developed use cases to allow end-users to effectively target data of interest. Key aspects of the architecture include a combined microservices and virtual machine deployment, provisioned through SaltStack and managed with Elastic and related tooling. While it is planned for the Data Center to reside at the NSO Headquarters, economies of scale are shifting, indicating a need to ensure “deploy-anywhere” (e.g., commercial cloud providers) can be supported effectively. The Data Center is currently completing its design phase, with development expected to occur in 2018-2020, with phased delivery of critical services occurring as DKIST comes online.
    <br /><br />
    When combined with a rigorous systems-engineering approach, including detailed requirements and interface controls, these three primary components will support DKIST use and scientific data exploitation. Despite the bespoke nature of the Summit CI, there is a significant focus on leveraging open source technologies in the DKIST, rather than relying on integration of commercial products. This is partly due to the long-term nature of the program and tight budgetary constraints. However, there are no free lunches – significant open source adoption without proactive forward replacement planning can leave obsolesced components underpinning critical systems. Given the long development timeline for the DKIST – the first CI work began in 2005 – these issues are already creeping into a yet-to-operate facility. Yet, the state of system development shows significant progress forward, and a bright future, for the DKIST CI.
    <br /><br />
    <h4>Summary</h4>
    This whitepaper briefly discusses the DKIST end-to-end cyberinfrastructure, focusing on the three primary entities and their roles. Each is in a different developmental state, emphasizing the importance of clear requirements and interfaces, effective team communication strategies, and stakeholder management.
  "

- name: "Gemini Observatory"
  website: "http://www.gemini.edu/"
  description: "
    <h4>Facility Description</h4>
    The Gemini Observatory consists of twin 8.1-meter diameter optical/infrared telescopes located on two of the best observing sites in the world: Maunakea in Hawaii and Cerro Pachon in Chile. From these two locations, Gemini’s telescopes can collectively provide access to the entire sky. Gemini was built and is operated by an international partnership of five countries including the United States, Canada, Brazil, Argentina and Chile. These Participants and the University of Hawaii, which has regular access to Gemini, each maintain a “National Gemini Office” to support their local users. Any astronomer in these countries can apply for time on Gemini, which is allocated in proportion to each Partcipant's financial stake. For the US, Gemini provides the largest publicly-accessible optical/infrared telescopes.
    <br /><br />
    Formally, the Mission Statement is “To advance our knowledge of the Universe by providing the international Gemini Community with forefront access to the entire sky.” Gemini’s achieves this by supporting peer-reviewed science proposed by the astronomical communities in the participating nations, and providing competitive instrumentation and observing modes in doing so. Over the five-year period between 2012 and 2016, more than 1000 individual Principal Investigators applied for Gemini observing time, from more than 300 academic institutions across the Gemini Partnership.
    <br /><br />
    <h4>Key products/services</h4>
    The direct product of Gemini observatory is observational data, taken in appropriate observing conditions, and placed in an archive for access by Principal Investigators (PIs). The service provided to PIs, jointly between the observatory and the NGOs, is to help prepare their observations, then to execute them on the telescopes or support the PI in executing them. Some PIs visit the telescope to make observations, others have their observations taken for them by staff operators. Gemini provides the preparation tool for PIs to create their observations. It also provides a data reduction package for all facility-class instruments. Currently this is based on the standard “IRAF” package distributed by NOAO.
    <br /><br />
    <h4>Facility CI</h4>
    The Gemini Observatory CI (computers, storage and networking; we do not include software in the definition) addresses the combined requirements of telescope operations, data handling and administrative support functions. Each of the four Gemini sites operates identical key services; a redundant core network service to support the distributed network environment, a redundant data storage system capable of replicating data offsite/cross-site in real time, a virtual machine cluster, a physical server farm, a virtual tape library backup environment, which also replicates data offsite, and instrumentation support infrastructure - such as per-instrument server hardware, network connectivity, remote power management and system monitoring.
    <br /><br />
    The two main Gemini sites (Gemini North and Gemini South) are connected via site-to-site VPN tunnels, that utilize the Internet 2 network infrastructure in the US, with interconnections to the REUNA research network in Chile.
    <br /><br />
    Additionally the two base facility sites in La Serena, Chile and Hilo, Hawaii are equipped with high power computers. These units offer Gemini scientist the possibility of efficiently processing data locally to support their research. While for the most part the consumption of these key services and components is separated, non-operational functions, such as research, project and document management, telecommunications and internet access, enjoy the benefits of increased redundancy and high availability.
    <br /><br />
    The median age of these key CI components is largely dictated by the manufacturers recommendations and enterprise support capabilities and experience in the field. These numbers are in turn transposed to the observatories longevity/obsolescence plan and are therefore understood in advance of the budget cycles. The networking equipment, for example, has a general operating age of around eight years, at which point the support contracts are no longer offered and spares are difficult to procure. The current core network hardware was replaced in 2014 and is set to be replaced in 2022. Similar examples can be made for each key CI component within Gemini, ensuring that the technology will also meet the observatory’s long term requirements.
  "

- name: "IceCube"
  website: "http://icecube.wisc.edu/"
  description: "
    IceCube is a neutrino detector built at the South Pole by instrumenting about a cubic kilometer of ice with 5160 light sensors. It uses Cherenkov light, emitted by charged particles moving through the ice to realize the enormous detection volume required for detecting neutrinos. One of the primary goals for IceCube is to elucidate the mechanisms for production of high-energy cosmic rays by detecting high-energy neutrinos from astrophysical sources. The Detector construction started in 2005 and finished in December 2010. Data taking started in 2006 and it is expected to be operated for at least 20 years. The United States National Science Foundation (NSF) supplied funds for the design, construction, and operations of the detector. As the host institution, the University of Wisconsin-Madison, with support from the NSF, has responsibility on the maintenance and operations of the detector. The scientific exploitation is carried out by an international Collaboration of about 300 researchers from 48 institutions in 12 countries.
    <br /><br />
    The IceCube data processing is divided in two regimes: online at the South Pole and offline at the UW-Madison main data processing center. Computing equipment is lifecycle replaced on average every ~4 years at the South Pole and ~5 years at UW-Madison. Several collaborating institutions also contribute to the offline computing infrastructure at different levels. Two Tier1 sites provide tape storage services for the long term preservation of the IceCube data products: NERSC in the US and DESY-Zeuthen in Germany. About 20 additional IceCube sites in the US, Canada, Europe and Asia provide computing resources for simulation and analysis.
    <br /><br />
    <h4>Online Computing Infrastructure</h4>
    Aggregation of data from the light sensors begins in the IceCube Laboratory (ICL), a central computing facility located on top of the detector hosting about 100 custom readout DOMHubs and 50 commodity servers. Data is collected from the array at a rate of 150 MB/s. After triggering and event building, the data is split into two independent paths. First, RAW data products are written to disks at a rate of about 1 TB/day, awaiting physical transfer north once per year. In addition, an online compute farm of 22 servers does near-real-time processing, event reconstruction, and filtering. Neutrino candidates and other event signatures of interest are identified within minutes, and notifications are dispatched to other astrophysical observatories worldwide via the Iridium satellite system. Approximately 100 GB/day of filtered events are queued for daily transmission to the main data processing facility at UW–Madison via high-bandwidth satellite links. Once in Madison, filtered data is further processed to a level suitable for scientific analysis.
    <br /><br />
    <h4>Offline Computing Infrastructure</h4>
    The main data processing facility at UW-Madison currently consists of ~7600 CPU cores, ~400 GPUs and ~6 PB of disk. This facility is used mainly for user analysis, but also for data processing and simulation production. Data products that need to be preserved for long time are replicated to two different locations: NERSC and DESY-Zeuthen.
    <br /><br />
    Conversion of event rates into physical fluxes ultimately relies on knowledge of detector characteristics numerically evaluated by running Monte Carlo simulations that model fundamental particle physics, the interaction of particles with matter, transport of optical photons through the ice, and detector response and electronics. Large amounts of simulations of background and signal events must be produced for use by the data analysts. The computationally expensive numerical models necessitate a distributed computing model that can make efficient use of a large number of clusters at many different locations.
    <br /><br />
    Up to 50% of the computing resources used by IceCube simulation and analysis are distributed (i.e. not at UW-Madison). The HTCondor software is used to federate these heterogeneous resources and present users a single consistent interface to all of them:
    <br /><br />
    <ul>
      <li>Local clusters at IceCube collaborating institutions</li>
      <li>UW campus shared clusters</li>
      <li>Open Science Grid</li>
      <li>XSEDE supercomputers</li>
    </ul>
  "

- name: "JOIDES Resolution Science Operator"
  website: "http://iodp.tamu.edu/"
  description: "
    The JOIDES Resolution Science Operator (JRSO) manages and operates the riserless drillship, JOIDES Resolution, for the International Ocean Discovery Program (IODP). The JRSO is based in the College of Geosciences at Texas A&M University.
    <br /><br />
    The JRSO is responsible for overseeing the science operations of the riserless drilling vessel JOIDES Resolution (JR), archiving the scientific data, samples and logs that are collected, and disseminated via web applications and online publications. The drillship travels throughout the oceans sampling the sediments and rocks beneath the seafloor. The scientific samples and data are used to study Earth’s past history, including plate tectonics, ocean currents, climate changes, evolutionary characteristics and extinctions of marine life, and mineral deposits.
    <br /><br />
    The JR is an NSF large facility that serves the global geosciences community. In addition to NSF funding through a cooperative agreement, JRSO operations are partly funded by 22 IODP member nations, including Australia, Austria, Brazil, Canada, China, Denmark, Finland, France, Germany, India, Ireland, Italy, Japan, Korea, Netherlands, New Zealand, Norway, Portugal, Spain, Sweden, Switzerland, and the United Kingdom.
    <br /><br />
    The cyberinfrastructure team supports a split based operations construct, providing cyberinfrastructure, cybersecurity and data management services at sea on board the JR and on shore in College Station, TX. VSAT (very small aperture terminal) satellite services are used to provide connectivity services between ship and shore. Currently, this is a dedicated asynchronous wide area network circuit offering 2 Mbps down to the ship and 1 Mbps up.
    <br /><br />
    The JRSO’s Laboratory Information Management System (LIMS) architecture (see picture below) is designed to capture, archive, process, manage, and disseminate data using several JRSO-developed instrument uploaders, client applications and web application tools. LIMS comprises the database that stores the data, the web services that pull and push the data, and the applications and hardware that capture and disseminate the data. One JRSO goal is to make this data, along with the data stored a legacy system (JANUS), more human and machine discoverable. JRSO is hopeful that the NSF-funded Open Core Data project will soon provide the data discovery capability it is seeking.
    <br /><br />
    The cyberinfrastructure team serves approximately 115 internal JRSO staff, 150 international scientists who sail on the JR each year, and the broader global geosciences community.
    <br /><br />
    Under its capital equipment replacement program, the JRSO routinely updates infrastructure services on ship and shore (i.e., servers, storage, backup services, battery backup, and high-speed network). The median age for JRSO infrastructure equipment is approximately six years.
    <br /><br />
    JRSO leverages Texas A&M University policies and tools to maintain its cybersecurity program. JRSO conducts a security self-assessment once per year using RSA Archer GRC in order to remain in compliance with university and state regulations.
    <br /><br />
    JRSO science data is permanently achieved at the NCEI facility in Boulder, CO.
  "

- name: "IRIS Data Services"
  website: "https://www.iris.edu/hq/"
  description: "
    The central component of IRIS Data Services (DS) is the IRIS Data Management Center in Seattle, Washington.  The DMC relies on other DS components in Albuquerque, La Jolla, University of Washington, LLNL, and Almaty, Kazakhstan to realize its full functionally but the heart of the DS is the DMC. The major CI components are in place at the DMC. We run a fully functional Auxiliary Data Center that is unmanned at LLNL.
    <br /><br />
    The IRIS DMC is a domain specific facility that meets the needs of the seismological community both within and outside the US. The DMC facilitates science within our domain but does not DO any science.
    <br /><br />
    Our science mission can be found in our <a href=\"http://www.iris.edu/hq/files/programs/data_services/policies/Strategic_Plan_v7.pdf\" target=\"_blank\">strategic plan</a>. Our science community numbers in the thousands worldwide.
    <br /><br />
    <div class=\"quote\">
      <strong>Mission:</strong> To provide reliable and efficient access to high quality seismological and related geophysical data, generated by IRIS and its domestic and international partners, and to enable all parties interested in using these data to do so in a straightforward and efficient manner.
      <br /><br />
      IRIS is university consortium with approximately 125 members (US academic institutions with graduate degrees in seismology) and roughly the same number of foreign affiliates scattered all over the globe. We are a 501c3 Delaware corporation.  We distribute primary data to roughly 25,000 (3rd level IP address) distinct users or IP addresses per quarter from roughly 12,000 distinct organizations (2nd level IP address). IRIS ingests roughly 75 terabytes of new observable data per year and we project we will more than one petabyte in 2017.
    </div>
    <br /><br />
    IRIS’ primary products are (Level 0, raw and Level 1 quality controlled) time series data. The time series come from roughly 30 types of sensors deployed on/in the ground, in the water column or water bottom, and in the atmosphere.  IRIS also produces Level 2 derived products, and manages community developed Level 2 and higher products. (See <a href=\"http://ds.iris.edu/spud/\" target=\"_blank\">http://ds.iris.edu/spud/</a>). Level 0 and 1 products are fully documented (metadata) time series data from geophysical sensors distributed globally generated form NSF and other national and international sources.  We distribute roughly one petabyte of level 0 and 1 data per year.
    <br /><br />
    Figure 1 shows volume of time series data shipped from the IRIS DMC to end users and or monitoring agencies since 2001. Major types of shipments include legacy requests in the blue, real time data distribution in the red, and web service distribution in the purple.
    <br /><br />
    IRIS also produces a great deal of community software and offers both IRIS developed and community developed software and tools in Redmine and GitHub repositories.  IRIS develops and maintains specific client applications for accessing and working with IRIS data.
    <br /><br />
    All IRIS data assets (Level 0-3) are available through service APIs.  Some of the APIs have been adopted internationally (FDSN web services) and other APIs are IRIS developed and maintained and not yet adopted internationally. (see <a href=\"http://service.iris.edu\" target=\"_blank\">http://service.iris.edu</a>). IRIS also maintains comprehensive documentation and is also the source of documentation for the SEED format, which is the international seismological domain format. (www.fdsn.org)
    <br /><br />
    The IRIS DMC operates a primary data center in Seattle as well as an unmanned, fully functional Auxiliary Data Center  (ADC) in Livermore California.  Major components of CI at the DMC and ADC consist of the following
    <br /><br />
    <ul>
      <li><strong>Storage –</strong> IRIS operates large volume Hitachi RAID systems that emphasis storage over performance.  We improve performance by indexing the RAID contents in a PostgreSql DBMS. We have roughly 700 terabytes of storage RAID at both the DMC and the ADC.  We also operate high performance RAID systems made by NetApp both for reception of real time data and PostgreSql database transactions.</li>
      <li><strong>Servers -</strong> IRIS runs virtual servers on physica Dell Servers. Virtualization software is VMWare. IRS operates Forcepoint Firewalls and A10 Load Balancers.  Load Balancers are configured so that a failure at the DMC or the ADC does not remove outsides user’s access to services,</li>
      <li><strong>LANs -</strong> We run 10 gigabit/second LANs sometimes in parallel to form a data backbone internal to the DMC and ADC. We connect to the Internet through the University of Washington.</li>
    <br /><br />
    Storage access to observational data has been abstracted through web services for both internal and external use. Access to data is transitioning from direct SQL access to abstractions thorugh web services. We are very close to running a SOA for both internal and external access.
    <br /><br />
    Our goal is to refresh all major computational and storage hardware infrastructure every four years. Budget pressues sometimes pushes this to 5 years.
    <br /><br />
    We are currently testing operating our software in XSEDE and AWS to see if this is viable.
  "

- name: "The NSF Cybersecurity Center of Excellence: Large Facilities Services"
  website: "http://trustedci.org/"
  description: "
    <h4>Overview of the NSF CCoE</h4>
    The genesis of the NSF Cybersecurity Center of Excellence (trustedci.org) is with a series of two workshops, the Scientific Software Security Innovation Institute (S3I2) workshops. The S3I2 workshops, held in 2010 [1] and 2011 [2] , included representatives of 35 major NSF-funded projects. The original goal of the workshops was to explore a software institute focused on IT security for the NSF community. What the workshops found is that the NSF community faces strong challenges in obtaining access to IT security expertise. Projects are forced to divert their resources to develop that expertise, address risks haphazardly, unknowingly reinvent basic cybersecurity solutions, and struggle with interoperability. The workshops further determined the need for access to expertise was more critical than any new software product. In 2012, based on these workshop findings, the NSF funded the Center for Trustworthy Scientific Cyberinfrastructure (CTSC) to provide security expertise to the NSF community. Building on the success of CTSC, the NSF Cybersecurity Center of Excellence (CCoE) was funded in 2016 as an expansion of the CTSC. The CCoE draws is a collaboration of four internationally recognized institutions: Indiana University, the University of Illinois, the University of Wisconsin-Madison, and the Pittsburgh Supercomputing Center.
    <br /><br />
    <h4>CCoE Services in support of Large Facilities</h4>
    Science projects manage a number of risks to their scientific missions including risks typically managed by cybersecurity, i.e. malicious entities who attack IT infrastructure to further their own ends at the expense of legitimate users or to explicitly harm those users. To be effective cybersecurity must be tailored for the science community, taking the community’s risks, tolerances, and technologies into account. The CCoE’s mission is to provide the NSF Large Facility community expertise in cybersecurity for science This mission is accomplished through one-on-one engagements with projects to address their specific challenges; education, outreach, and training to raise the state of security practice across the scientific enterprise; and leadership in advancing the overall state of knowledge on cybersecurity for science through applied research and community building. Examples of these mechanisms follow. Details can be found on trustedci.org.
    <br /><br />
    <strong>One-on-one engagements:</strong>
    <br /><br />
    <ul>
      <li>DKIST: DKIST and the CCoE collaborated to develop a cybersecurity planning guide for DKIST that addresses these terms and conditions, aligns with existing institutional policies, and can be implemented within DKIST’s budgetary limitations. This guide was made generally available for other NSF large facilities and projects [3].</li>
      <li>LIGO: The CCoE, LIGO and the Open Science Grid collaborated to establish an international identity federation in support of LIGO's scientific mission.</li>
      <li>Icecube, LSST, NEON: The CCoE helped with the development, assessment, and improvement of operational cybersecurity programs.</li>
      <li>Globus, Pegasus, OSG: The CCoE provided software security consulting and assurance evaluation to helping the NSF community develop more secure software and assess software they are using (or considering using).</li>
    </ul>
    <br /><br />
    <strong>Education, outreach and training:</strong>
    <br /><br />
    <ul>
      <li>Situational awareness: The CCoE provides situational awareness of the current cyber threats to the research and education environment, including those that impact scientific instruments, by providing timely email notifications about relevant software vulnerabilities.</li>
      <li>Webinars: The CCoE offers a monthly webinar series to allow NSF projects to share findings and experiences with each other.</li>
      <li>Training: The CCoE regularly provides training, tailored to the science community, on a number on a number of topics, including log analysis, incident response, federated identity management, and developing a cybersecurity program.</li>
    </ul>
    <br /><br />
    <strong>Advancing the state of knowledge through applied research and community building:</strong>
    <br /><br />
    <ul>
      <li>Large Facility Security Working Group: to develop a working relationship between those responsible for cybersecurity across the LFs and to advance the development and implementation of best practices, standards and requirements within the community.</li>
      <li>NSF Cybersecurity Summit for Large Facilities and Cyberinfrastructure:The CCoE organizes this annual event to bring together leaders in NSF cyberinfrastructure and cybersecurity to build a trusting, collaborative community, and to address that community's core cybersecurity challenges.</li>
    </ul>
    <br /><br />
    <h4>References</h4>
    <ul>
      <li>[1] William Barnett, Jim Basney, Randy Butler, and Doug Pearson, “Report on the NSF Workshop on Scientific Software Security Innovation Institute (S3I2) (2010),” Oct. 2010 [Online]. Available: https://security.ncsa.illinois.edu/s3i2/s3i2-workshop-final-report.pdf</li>
      <li>[2] William Barnett, Jim Basney, Randy Butler, and Doug Pearson, “Report of NSF Workshop Series on Scientific Software Security Innovation Institute (S3I2) (2011),” Oct. 2010 [Online]. Available: https://security.ncsa.illinois.edu/s3i2/S3I2WorkshopReport2011Final.pdf</li>
      <li>[3] Jim Marsteller, Craig Jackson, Susan Sons, Jared Allar, Terry Fleury, Patrick Duda, “Guide to Developing Cybersecurity Programs for NSF Science and Engineering Projects, v1,” Center for Trustworthy Scientific Cyberinfrastructure, Aug. 2014 [Online]. Available: https://scholarworks.iu.edu/dspace/handle/2022/20026 . [Accessed: 18-Jun-2017]</li>
    </ul>
  "
